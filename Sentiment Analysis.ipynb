{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U nltk scikit-learn pandas matplotlib numpy wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reviews = pd.read_csv('./files/imdb-reviews.csv')\n",
    "reviews['sentiment'] = reviews['sentiment'].map({'neg': 0, 'pos': 1})\n",
    "reviews.drop(columns=[\"text_pt\", \"id\"], inplace=True)\n",
    "reviews.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cleaning:\n",
    "    Clear and get only the main part from the dataset\n",
    "    Ex: remove the tags of the html.\n",
    "    Ex: filter the texts in PDF and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # Remove the HTML tags\n",
    "    text = re.sub(\"<!--?.*?-->\",\"\",text)\n",
    "    text = re.sub(\"<.*?>\",\"\",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalization:\n",
    "    Remove the pontuation, tags, put everything in same case and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\", \" \",text)\n",
    "    \n",
    "    text = text.replace('  ', ' ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenization:\n",
    "    Split the text in words spliting by the whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):    \n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stop Words:\n",
    "    They are words witch don't get no one meaning, they are just used to complement the context,\n",
    "    and to connect the terms.\n",
    "    Ex: 'i', 'you', 'in', 'out', 'are', 'the'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove the stop words, they are words witch don't give no one especific meaning\n",
    "def remove_stopwords(tokens):\n",
    "    return [w for w in tokens if w not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Stemming:\n",
    "    Takes of the variation of the words and remove the finally to combine than.\n",
    "    Ex: 'change', 'changing', 'changes' => 'chang'\n",
    "\n",
    "\n",
    "### 6. Lemmatization:\n",
    "    Takes the variation of the same word and convert to the same one (Noun).\n",
    "    Ex: 'is', 'were', 'was' => 'be'\n",
    "    Ex: 'ones' => 'one'\n",
    "\n",
    "    Part of Speech(PoS) (Verb):\n",
    "    Ex: 'bored' => 'bore'\n",
    "    Ex: 'stating' => 'start'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Remove the pural\n",
    "# Remove the verb conjugation\n",
    "def stem(words):\n",
    "    return [PorterStemmer().stem(w) for w in words]\n",
    "\n",
    "# Remove the personality\n",
    "def lem(words):\n",
    "    return [WordNetLemmatizer().lemmantize(w, ) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Tag filtering\n",
    "    Filter the words according with the sintaxe definition like a noun, verbs, adverbs e etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# RB | RB | JJ | NN | NNP | JJ | JJS | IN | VB | VBZ | VBD | VBG\n",
    "\n",
    "# IN = preposition/subordinating conjunction\n",
    "\n",
    "# RB = adverb very, silently\n",
    "# RBR = adverb, comparative better\n",
    "# RBS = adverb, superlative best\n",
    "# RP = particle give up\n",
    "\n",
    "# IN = preposition/subordinating conjunction\n",
    "# JJ = adjective ‘big’\n",
    "# JJR = adjective, comparative ‘bigger’\n",
    "# JJS = adjective, superlative ‘biggest’\n",
    "\n",
    "# VB verb, base form take\n",
    "# VBD verb, past tense took\n",
    "# VBG verb, gerund/present participle taking\n",
    "# VBN verb, past participle taken\n",
    "# VBP verb, sing. present, non-3d take\n",
    "# VBZ verb, 3rd person sing. present takes\n",
    "\n",
    "# Filter using regular array\n",
    "def filter_tokens(tokens):\n",
    "    tags = [x[1] for x in nltk.pos_tag(list(tokens))]\n",
    "    filters = (\"RB\", \"RBR\", \"RBS\", \"RP\", \"JJ\", \"JJR\", \"JJS\", \"JJ\", \"VB\")\n",
    "    \n",
    "    return [tokens[i] for i in range(len(tokens)) if tags[i] in filters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = \" \".join(list(reviews.text_en[:1000]))\n",
    "\n",
    "tokens = tokenizer(normalize(clean(all_words)))\n",
    "print(\"> Tokenized!\")\n",
    "\n",
    "tokens = remove_stopwords(tokens)\n",
    "print(\"> Removed the stop words!\")\n",
    "\n",
    "tokens = stem(tokens)\n",
    "# tokens = lem(tokens)\n",
    "print(\"> Merged the term by stem or lem!\")\n",
    "\n",
    "tokens = filter_tokens(tokens)\n",
    "print(\"> Filtred by tags witch get more meaning!\")\n",
    "\n",
    "print(\"\\nColection [:100]:\\n\")\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the frequency of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "frequency = nltk.FreqDist(tokens)\n",
    "\n",
    "# Create the bag of words dataframe\n",
    "bag_of_words = pd.DataFrame({\"words\": list(frequency.keys()), \"frequency\": list(frequency.values())})\n",
    "\n",
    "# Order by the Frequency\n",
    "bag_of_words.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "bag_of_words.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the bag of words\n",
    "bag_of_words.to_csv('./files/bag-of-words.csv', index=True)\n",
    "\n",
    "print(f\"Back of words size: {bag_of_words.shape[0]}\")\n",
    "\n",
    "print(bag_of_words.shape[0])\n",
    "bag_of_words.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the frequency in Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_single_str(words, frequency):\n",
    "    words = list(words)\n",
    "    frequency = list(frequency)\n",
    "    \n",
    "    return \" \".join([(words[i] + \" \") * frequency[i] for i in range(len(frequency))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_cloud = WordCloud(width=800, height=500, max_font_size=110, collocations=False).generate(to_single_str(bag_of_words[\"words\"], bag_of_words[\"frequency\"]))\n",
    "\n",
    "plt.figure(figsize=(13, 13))\n",
    "plt.imshow(word_cloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictinary\n",
    "bag_of_words = pd.read_csv('./files/bag-of-words.csv')\n",
    "bag_of_words_array = bag_of_words.words.values\n",
    "\n",
    "# Get the inputs\n",
    "reviews = pd.read_csv('./files/imdb-reviews.csv')\n",
    "reviews['sentiment'] = reviews['sentiment'].map({'neg': 0, 'pos': 1})\n",
    "reviews.drop(columns=[\"text_pt\", \"id\"], inplace=True)\n",
    "\n",
    "inputs = reviews.text_en.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def tf_idf(txt, vocabulary=None):\n",
    "    txt = list(txt)\n",
    "\n",
    "    tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word', vocabulary=vocabulary)\n",
    "    txt_transformed = tf.fit(txt).transform(txt)\n",
    "\n",
    "    return pd.DataFrame(txt_transformed.toarray(), columns=tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tf_idf(inputs, bag_of_words_array)\n",
    "tfidf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = tfidf.values\n",
    "y = [[x] for x in reviews.sentiment.values]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "model = DummyClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "accuracy = model.score(x_test, y_test) * 100\n",
    "print(\"Taxa de acerto do algoritimo de Base line: %.2f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "# Test a linear model\n",
    "model = LinearSVC()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "accuracy = model.score(x_test, y_test) * 100\n",
    "print(\"Linear SVC accuracy: %.2f%%\" % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
