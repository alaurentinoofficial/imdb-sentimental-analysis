{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "Requirement already up-to-date: scikit-learn in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (0.21.3)\n",
      "Collecting pandas\n",
      "  Downloading https://files.pythonhosted.org/packages/af/b2/51c178d516b85be51f3a3bd30c654453a3884a34d6329343555418b5d7cb/pandas-0.25.1-cp36-cp36m-win_amd64.whl (9.0MB)\n",
      "Requirement already up-to-date: matplotlib in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (3.1.1)\n",
      "Requirement already up-to-date: numpy in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (1.17.0)\n",
      "Requirement already up-to-date: wordcloud in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from nltk) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from scikit-learn) (0.13.2)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from pandas) (2018.4)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from matplotlib) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pillow in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from wordcloud) (5.1.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449382 sha256=6ac15f1e836a4f87d44c7a719079e4fd928d324f481f6e3cbaa99aa2fac9bd1f\n",
      "  Stored in directory: C:\\Users\\a.lima.laurentino\\AppData\\Local\\pip\\Cache\\wheels\\96\\86\\f6\\68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk, pandas\n",
      "  Found existing installation: nltk 3.4.3\n",
      "    Uninstalling nltk-3.4.3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\program files (x86)\\\\microsoft visual studio\\\\shared\\\\anaconda3_64\\\\lib\\\\site-packages\\\\nltk-3.4.3.dist-info\\\\INSTALLER'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: You are using pip version 19.2.1, however version 19.2.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk scikit-learn pandas matplotlib numpy wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_en</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brass pictures movies is not a fitting word fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A funny thing happened to me while watching \"M...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This German horror film has to be one of the w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Being a long-time fan of Japanese film, I expe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Tokyo Eyes\" tells of a 17 year old Japanese g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wealthy horse ranchers in Buenos Aires have a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_en  sentiment\n",
       "0  Once again Mr. Costner has dragged out a movie...          0\n",
       "1  This is an example of why the majority of acti...          0\n",
       "2  First of all I hate those moronic rappers, who...          0\n",
       "3  Not even the Beatles could write songs everyon...          0\n",
       "4  Brass pictures movies is not a fitting word fo...          0\n",
       "5  A funny thing happened to me while watching \"M...          0\n",
       "6  This German horror film has to be one of the w...          0\n",
       "7  Being a long-time fan of Japanese film, I expe...          0\n",
       "8  \"Tokyo Eyes\" tells of a 17 year old Japanese g...          0\n",
       "9  Wealthy horse ranchers in Buenos Aires have a ...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reviews = pd.concat([pd.read_csv('./files/reviews_en-pt_1.csv'), pd.read_csv('./files/reviews_en-pt_2.csv')], join='outer')\n",
    "reviews['sentiment'] = reviews['sentiment'].map({'neg': 0, 'pos': 1})\n",
    "reviews.drop(columns=[\"text_pt\", \"id\"], inplace=True)\n",
    "reviews.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cleaning:\n",
    "    Clear and get only the main part from the dataset\n",
    "    Ex: remove the tags of the html.\n",
    "    Ex: filter the texts in PDF and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # Remove the HTML tags\n",
    "    text = re.sub(\"<!--?.*?-->\",\"\",text)\n",
    "    text = re.sub(\"<.*?>\",\"\",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalization:\n",
    "    Remove the pontuation, tags, put everything in same case and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\", \" \",text)\n",
    "    \n",
    "    text = text.replace('  ', ' ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenization:\n",
    "    Split the text in words spliting by the whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):    \n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stop Words:\n",
    "    They are words witch don't get no one meaning, they are just used to complement the context,\n",
    "    and to connect the terms.\n",
    "    Ex: 'i', 'you', 'in', 'out', 'are', 'the'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove the stop words, they are words witch don't give no one especific meaning\n",
    "def remove_stopwords(tokens):\n",
    "    return [w for w in tokens if w not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Stemming:\n",
    "    Takes of the variation of the words and remove the finally to combine than.\n",
    "    Ex: 'change', 'changing', 'changes' => 'chang'\n",
    "\n",
    "\n",
    "### 6. Lemmatization:\n",
    "    Takes the variation of the same word and convert to the same one (Noun).\n",
    "    Ex: 'is', 'were', 'was' => 'be'\n",
    "    Ex: 'ones' => 'one'\n",
    "\n",
    "    Part of Speech(PoS) (Verb):\n",
    "    Ex: 'bored' => 'bore'\n",
    "    Ex: 'stating' => 'start'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Remove the pural\n",
    "# Remove the verb conjugation\n",
    "def stem(words):\n",
    "    return [PorterStemmer().stem(w) for w in words]\n",
    "\n",
    "# Remove the personality\n",
    "def lem(words):\n",
    "    return [WordNetLemmatizer().lemmantize(w, ) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Tag filtering\n",
    "    Filter the words according with the sintaxe definition like a noun, verbs, adverbs e etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# RB | RB | JJ | NN | NNP | JJ | JJS | IN | VB | VBZ | VBD | VBG\n",
    "\n",
    "# IN = preposition/subordinating conjunction\n",
    "\n",
    "# RB = adverb very, silently\n",
    "# RBR = adverb, comparative better\n",
    "# RBS = adverb, superlative best\n",
    "# RP = particle give up\n",
    "\n",
    "# IN = preposition/subordinating conjunction\n",
    "# JJ = adjective ‘big’\n",
    "# JJR = adjective, comparative ‘bigger’\n",
    "# JJS = adjective, superlative ‘biggest’\n",
    "\n",
    "# VB verb, base form take\n",
    "# VBD verb, past tense took\n",
    "# VBG verb, gerund/present participle taking\n",
    "# VBN verb, past participle taken\n",
    "# VBP verb, sing. present, non-3d take\n",
    "# VBZ verb, 3rd person sing. present takes\n",
    "\n",
    "# Filter using regular array\n",
    "def filter_tokens(tokens):\n",
    "    tags = [x[1] for x in nltk.pos_tag(list(tokens))]\n",
    "    filters = (\"RB\", \"RBR\", \"RBS\", \"RP\", \"JJ\", \"JJR\", \"JJS\", \"JJ\", \"VB\")\n",
    "    \n",
    "    return [tokens[i] for i in range(len(tokens)) if tags[i] in filters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Tokenized!\n",
      "> Removed the stop words!\n",
      "> Merged the term by stem or lem!\n",
      "> Filtred by tags witch get more meaning!\n",
      "\n",
      "Colection [:100]:\n",
      "\n",
      "['far', 'longer', 'necessari', 'charact', 'forgotten', 'much', 'later', 'better', 'sign', 'win', 'final', 'half', 'kutcher', 'best', 'prior', 'keep', 'turn', 'major', 'realli', 'worth', 'tap', 'proven', 'mani', 'well', 'dont', 'bother', 'go', 'see', 'new', 'new', 'undercov', 'n', 'higher', 'friday', 'real', 'clich', 'still', 'wonder', 'alway', 'play', 'exact', 'charact', 'alien', 'ive', 'exact', 'irrit', 'least', 'alien', 'somewhat', 'gratifi', 'overal', 'second', 'better', 'see', 'practic', 'better', 'better', 'script', 'worth', 'decent', 'almost', 'refresh', 'close', 'first', 'hate', 'gun', 'go', 'alreadi', 'warehous', 'also', 'sadler', 'much', 'right', 'peopl', 'everywher', 'pretti', 'much', 'big', 'get', 'deserv', 'black', 'ugli', 'dead', 'stay', 'away', 'crap', 'instead', 'lest', 'real', 'even', 'write', 'song', 'mop', 'top', 'provok', 'social', 'movi', 'full', 'back', 'seat']\n"
     ]
    }
   ],
   "source": [
    "all_words = \" \".join(list(reviews.text_en[:1000]))\n",
    "\n",
    "tokens = tokenizer(normalize(clean(all_words)))\n",
    "print(\"> Tokenized!\")\n",
    "\n",
    "tokens = remove_stopwords(tokens)\n",
    "print(\"> Removed the stop words!\")\n",
    "\n",
    "tokens = stem(tokens)\n",
    "# tokens = lem(tokens)\n",
    "print(\"> Merged the term by stem or lem!\")\n",
    "\n",
    "tokens = filter_tokens(tokens)\n",
    "print(\"> Filtred by tags witch get more meaning!\")\n",
    "\n",
    "print(\"\\nColection [:100]:\\n\")\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the frequency of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back of words size: 5190\n",
      "5190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bad</td>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>even</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>movi</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>much</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  words  frequency\n",
       "0   bad        648\n",
       "1  even        633\n",
       "2  good        532\n",
       "3  movi        430\n",
       "4  much        387"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "frequency = nltk.FreqDist(tokens)\n",
    "\n",
    "# Create the bag of words dataframe\n",
    "bag_of_words = pd.DataFrame({\"words\": list(frequency.keys()), \"frequency\": list(frequency.values())})\n",
    "\n",
    "# Order by the Frequency\n",
    "bag_of_words.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "bag_of_words.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the bag of words\n",
    "bag_of_words.to_csv('./files/bag-of-words.csv', index=True)\n",
    "\n",
    "print(f\"Back of words size: {bag_of_words.shape[0]}\")\n",
    "\n",
    "print(bag_of_words.shape[0])\n",
    "bag_of_words.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the frequency in Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_single_str(words, frequency):\n",
    "    words = list(words)\n",
    "    frequency = list(frequency)\n",
    "    \n",
    "    return \" \".join([(words[i] + \" \") * frequency[i] for i in range(len(frequency))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1300x1300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_cloud = WordCloud(width=800, height=500, max_font_size=110, collocations=False).generate(to_single_str(bag_of_words[\"words\"], bag_of_words[\"frequency\"]))\n",
    "\n",
    "plt.figure(figsize=(13, 13))\n",
    "plt.imshow(word_cloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictinary\n",
    "bag_of_words = pd.read_csv('./files/bag-of-words.csv')\n",
    "bag_of_words_array = bag_of_words.words.values\n",
    "\n",
    "# Get the inputs\n",
    "reviews = pd.concat([pd.read_csv('./files/reviews_en-pt_1.csv'), pd.read_csv('./files/reviews_en-pt_2.csv')], join='outer')\n",
    "reviews['sentiment'] = reviews['sentiment'].map({'neg': 0, 'pos': 1})\n",
    "reviews.drop(columns=[\"text_pt\", \"id\"], inplace=True)\n",
    "\n",
    "inputs = reviews.text_en.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def tf_idf(txt, vocabulary=None):\n",
    "    txt = list(txt)\n",
    "\n",
    "    tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word', vocabulary=vocabulary)\n",
    "    txt_transformed = tf.fit(txt).transform(txt)\n",
    "\n",
    "    return pd.DataFrame(txt_transformed.toarray(), columns=tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1278: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>even</th>\n",
       "      <th>good</th>\n",
       "      <th>movi</th>\n",
       "      <th>much</th>\n",
       "      <th>get</th>\n",
       "      <th>well</th>\n",
       "      <th>first</th>\n",
       "      <th>better</th>\n",
       "      <th>ever</th>\n",
       "      <th>...</th>\n",
       "      <th>flail</th>\n",
       "      <th>copyright</th>\n",
       "      <th>kenni</th>\n",
       "      <th>yakkel</th>\n",
       "      <th>profan</th>\n",
       "      <th>loney</th>\n",
       "      <th>quench</th>\n",
       "      <th>outlin</th>\n",
       "      <th>choic</th>\n",
       "      <th>reliev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.286652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.212528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.701125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.212528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.103376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.573304</td>\n",
       "      <td>2.337175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.398623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.112969</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.212528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.97721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.286652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.212528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.97721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.286652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.657334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.674350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.112969</td>\n",
       "      <td>1.97721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 5190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bad      even     good  movi      much       get      well     first  \\\n",
       "0  0.0  0.000000  0.00000   0.0  2.286652  0.000000  2.212528  0.000000   \n",
       "1  0.0  0.000000  0.00000   0.0  0.000000  0.000000  2.212528  0.000000   \n",
       "2  0.0  0.000000  0.00000   0.0  4.573304  2.337175  0.000000  2.398623   \n",
       "3  0.0  2.112969  0.00000   0.0  0.000000  0.000000  2.212528  0.000000   \n",
       "4  0.0  0.000000  1.97721   0.0  2.286652  0.000000  2.212528  0.000000   \n",
       "5  0.0  0.000000  0.00000   0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.0  0.000000  0.00000   0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.0  0.000000  1.97721   0.0  2.286652  0.000000  0.000000  0.000000   \n",
       "8  0.0  0.000000  0.00000   0.0  0.000000  4.674350  0.000000  0.000000   \n",
       "9  0.0  2.112969  1.97721   0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "     better      ever  ...  flail  copyright  kenni  yakkel  profan  loney  \\\n",
       "0  2.701125  0.000000  ...    0.0        0.0    0.0     0.0     0.0    0.0   \n",
       "1  8.103376  0.000000  ...    0.0        0.0    0.0     0.0     0.0    0.0   \n",
       "2  0.000000  0.000000  ...    0.0        0.0    0.0     0.0     0.0    0.0   \n",
       "3  0.000000  0.000000  ...    0.0        0.0    0.0     0.0     0.0    0.0   \n",
       "4  0.000000  0.000000  ...    0.0        0.0    0.0     0.0     0.0    0.0   \n",
       "5  0.000000  0.000000  ...    0.0        0.0    0.0     0.0     0.0    0.0   \n",
       "6  0.000000  0.000000  ...    0.0        0.0    0.0     0.0     0.0    0.0   \n",
       "7  0.000000  2.657334  ...    0.0        0.0    0.0     0.0     0.0    0.0   \n",
       "8  0.000000  0.000000  ...    0.0        0.0    0.0     0.0     0.0    0.0   \n",
       "9  0.000000  0.000000  ...    0.0        0.0    0.0     0.0     0.0    0.0   \n",
       "\n",
       "   quench  outlin  choic  reliev  \n",
       "0     0.0     0.0    0.0     0.0  \n",
       "1     0.0     0.0    0.0     0.0  \n",
       "2     0.0     0.0    0.0     0.0  \n",
       "3     0.0     0.0    0.0     0.0  \n",
       "4     0.0     0.0    0.0     0.0  \n",
       "5     0.0     0.0    0.0     0.0  \n",
       "6     0.0     0.0    0.0     0.0  \n",
       "7     0.0     0.0    0.0     0.0  \n",
       "8     0.0     0.0    0.0     0.0  \n",
       "9     0.0     0.0    0.0     0.0  \n",
       "\n",
       "[10 rows x 5190 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = tf_idf(inputs, bag_of_words_array)\n",
    "tfidf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = tfidf.values\n",
    "y = [[x] for x in reviews.sentiment.values]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxa de acerto do algoritimo de Base line: 49.89%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "model = DummyClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "accuracy = model.score(x_test, y_test) * 100\n",
    "print(\"Taxa de acerto do algoritimo de Base line: %.2f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC accuracy: 79.22%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "# Test a linear model\n",
    "model = LinearSVC()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "accuracy = model.score(x_test, y_test) * 100\n",
    "print(\"Linear SVC accuracy: %.2f%%\" % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
